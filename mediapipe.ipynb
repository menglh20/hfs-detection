{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "mp_draw_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, face_landmarks):\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Draw the face landmarks.\n",
    "    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    face_landmarks_proto.landmark.extend([\n",
    "        landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "    ])\n",
    "    # # 人脸网格\n",
    "    # solutions.drawing_utils.draw_landmarks(\n",
    "    #     image=annotated_image,\n",
    "    #     landmark_list=face_landmarks_proto,\n",
    "    #     connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "    #     landmark_drawing_spec=None,\n",
    "    #     connection_drawing_spec=mp.solutions.drawing_styles\n",
    "    #     .get_default_face_mesh_tesselation_style())\n",
    "    # # 脸廓、眼睫毛、眼眶、嘴唇\n",
    "    # solutions.drawing_utils.draw_landmarks(\n",
    "    #     image=annotated_image,\n",
    "    #     landmark_list=face_landmarks_proto,\n",
    "    #     connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "    #     landmark_drawing_spec=None,\n",
    "    #     connection_drawing_spec=mp.solutions.drawing_styles\n",
    "    #     .get_default_face_mesh_contours_style())\n",
    "    # # 瞳孔\n",
    "    # solutions.drawing_utils.draw_landmarks(\n",
    "    #     image=annotated_image,\n",
    "    #     landmark_list=face_landmarks_proto,\n",
    "    #     connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "    #     landmark_drawing_spec=None,\n",
    "    #     connection_drawing_spec=mp.solutions.drawing_styles\n",
    "    #     .get_default_face_mesh_iris_connections_style())\n",
    "    # 关键点\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        landmark_drawing_spec=mp_draw.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=1))\n",
    "\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_eye = [33, 246, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 7, 33]\n",
    "right_eye = [263, 466, 388, 387, 386, 385, 384, 398, 362, 382, 381, 380, 374, 373, 390, 249]\n",
    "mouth = [61, 185, 40, 39, 37, 0, 267, 270, 409, 291, 375, 321, 405, 314, 17, 84, 181, 91, 146]\n",
    "eye_center = [468, 473]\n",
    "mid = [9, 8, 168, 6, 197, 195, 5, 4, 1, 19, 2, 164, 0, 11, 12, 13, 14, 15, 16, 17, 18, 200]\n",
    "contours = [234, 454]\n",
    "\n",
    "def process_face_landmarks(face_landmarks):\n",
    "    processed_face_landmarks = []\n",
    "    for idx in left_eye:\n",
    "        processed_face_landmarks.append(face_landmarks[idx])\n",
    "    for idx in right_eye:\n",
    "        processed_face_landmarks.append(face_landmarks[idx])\n",
    "    for idx in mouth:\n",
    "        processed_face_landmarks.append(face_landmarks[idx])\n",
    "    for idx in eye_center:\n",
    "        processed_face_landmarks.append(face_landmarks[idx])\n",
    "    for idx in mid:\n",
    "        processed_face_landmarks.append(face_landmarks[idx])\n",
    "    for idx in contours:\n",
    "        processed_face_landmarks.append(face_landmarks[idx])\n",
    "    return processed_face_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video FPS: 60.00454249504111\n",
      "Total Frames: 258\n",
      "Video Resolution: 1920x1080\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "FaceLandmarker = mp.tasks.vision.FaceLandmarker\n",
    "FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "\n",
    "# Create a face landmarker instance with the video mode:\n",
    "model_path = 'face_landmarker.task'\n",
    "options = FaceLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.VIDEO)\n",
    "\n",
    "\n",
    "with FaceLandmarker.create_from_options(options) as landmarker:\n",
    "    # 打开视频文件\n",
    "    video_path = '../data/processed/L1/6-ce.mp4'  # 替换为你的视频文件路径\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # 获取视频的帧率和总帧数\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    h, w = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "    print(f\"Video FPS: {fps}\")\n",
    "    print(f\"Total Frames: {total_frames}\")\n",
    "    print(f\"Video Resolution: {w}x{h}\")\n",
    "\n",
    "    n = 1\n",
    "    # 读取并显示每一帧\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        n += 1\n",
    "\n",
    "        # 检查是否成功读取帧\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "        face_landmarker_result = landmarker.detect_for_video(mp_image, math.floor(n * 1000 / fps))\n",
    "        face_landmarks = face_landmarker_result.face_landmarks[0]\n",
    "\n",
    "        # for landmark in face_landmarks:\n",
    "        #     cx = int(landmark.x * w)\n",
    "        #     cy = int(landmark.y * h)\n",
    "        #     print(landmark.x, landmark.y, landmark.z)\n",
    "        #     print(cx, cy)\n",
    "\n",
    "        processed_face_landmarks = process_face_landmarks(face_landmarks)\n",
    "        annotated_image = draw_landmarks_on_image(frame, processed_face_landmarks)\n",
    "        cv2.imshow('Video Frame', annotated_image)\n",
    "\n",
    "        # break\n",
    "        # 按 'q' 键退出循环\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # 释放资源\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on FaceLandmarker in module mediapipe.tasks.python.vision.face_landmarker object:\n",
      "\n",
      "class FaceLandmarker(mediapipe.tasks.python.vision.core.base_vision_task_api.BaseVisionTaskApi)\n",
      " |  FaceLandmarker(graph_config: mediapipe.framework.calculator_pb2.CalculatorGraphConfig, running_mode: mediapipe.tasks.python.vision.core.vision_task_running_mode.VisionTaskRunningMode, packet_callback: Optional[Callable[[Mapping[str, mediapipe.python._framework_bindings.packet.Packet]], NoneType]] = None) -> None\n",
      " |  \n",
      " |  Class that performs face landmarks detection on images.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FaceLandmarker\n",
      " |      mediapipe.tasks.python.vision.core.base_vision_task_api.BaseVisionTaskApi\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  detect(self, image: mediapipe.python._framework_bindings.image.Image, image_processing_options: Optional[mediapipe.tasks.python.vision.core.image_processing_options.ImageProcessingOptions] = None) -> mediapipe.tasks.python.vision.face_landmarker.FaceLandmarkerResult\n",
      " |      Performs face landmarks detection on the given image.\n",
      " |      \n",
      " |      Only use this method when the FaceLandmarker is created with the image\n",
      " |      running mode.\n",
      " |      \n",
      " |      The image can be of any size with format RGB or RGBA.\n",
      " |      TODO: Describes how the input image will be preprocessed after the yuv\n",
      " |      support is implemented.\n",
      " |      \n",
      " |      Args:\n",
      " |        image: MediaPipe Image.\n",
      " |        image_processing_options: Options for image processing.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The face landmarks detection results.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If any of the input arguments is invalid.\n",
      " |        RuntimeError: If face landmarker detection failed to run.\n",
      " |  \n",
      " |  detect_async(self, image: mediapipe.python._framework_bindings.image.Image, timestamp_ms: int, image_processing_options: Optional[mediapipe.tasks.python.vision.core.image_processing_options.ImageProcessingOptions] = None) -> None\n",
      " |      Sends live image data to perform face landmarks detection.\n",
      " |      \n",
      " |      The results will be available via the \"result_callback\" provided in the\n",
      " |      FaceLandmarkerOptions. Only use this method when the FaceLandmarker is\n",
      " |      created with the live stream running mode.\n",
      " |      \n",
      " |      Only use this method when the FaceLandmarker is created with the live\n",
      " |      stream running mode. The input timestamps should be monotonically increasing\n",
      " |      for adjacent calls of this method. This method will return immediately after\n",
      " |      the input image is accepted. The results will be available via the\n",
      " |      `result_callback` provided in the `FaceLandmarkerOptions`. The\n",
      " |      `detect_async` method is designed to process live stream data such as\n",
      " |      camera input. To lower the overall latency, face landmarker may drop the\n",
      " |      input images if needed. In other words, it's not guaranteed to have output\n",
      " |      per input image.\n",
      " |      \n",
      " |      The `result_callback` provides:\n",
      " |        - The face landmarks detection results.\n",
      " |        - The input image that the face landmarker runs on.\n",
      " |        - The input timestamp in milliseconds.\n",
      " |      \n",
      " |      Args:\n",
      " |        image: MediaPipe Image.\n",
      " |        timestamp_ms: The timestamp of the input image in milliseconds.\n",
      " |        image_processing_options: Options for image processing.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the current input timestamp is smaller than what the\n",
      " |        face landmarker has already processed.\n",
      " |  \n",
      " |  detect_for_video(self, image: mediapipe.python._framework_bindings.image.Image, timestamp_ms: int, image_processing_options: Optional[mediapipe.tasks.python.vision.core.image_processing_options.ImageProcessingOptions] = None) -> mediapipe.tasks.python.vision.face_landmarker.FaceLandmarkerResult\n",
      " |      Performs face landmarks detection on the provided video frame.\n",
      " |      \n",
      " |      Only use this method when the FaceLandmarker is created with the video\n",
      " |      running mode.\n",
      " |      \n",
      " |      Only use this method when the FaceLandmarker is created with the video\n",
      " |      running mode. It's required to provide the video frame's timestamp (in\n",
      " |      milliseconds) along with the video frame. The input timestamps should be\n",
      " |      monotonically increasing for adjacent calls of this method.\n",
      " |      \n",
      " |      Args:\n",
      " |        image: MediaPipe Image.\n",
      " |        timestamp_ms: The timestamp of the input video frame in milliseconds.\n",
      " |        image_processing_options: Options for image processing.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The face landmarks detection results.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If any of the input arguments is invalid.\n",
      " |        RuntimeError: If face landmarker detection failed to run.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  create_from_model_path(model_path: str) -> 'FaceLandmarker' from builtins.type\n",
      " |      Creates an `FaceLandmarker` object from a TensorFlow Lite model and the default `FaceLandmarkerOptions`.\n",
      " |      \n",
      " |      Note that the created `FaceLandmarker` instance is in image mode, for\n",
      " |      detecting face landmarks on single image inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        model_path: Path to the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `FaceLandmarker` object that's created from the model file and the\n",
      " |        default `FaceLandmarkerOptions`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If failed to create `FaceLandmarker` object from the\n",
      " |          provided file such as invalid file path.\n",
      " |        RuntimeError: If other types of error occurred.\n",
      " |  \n",
      " |  create_from_options(options: mediapipe.tasks.python.vision.face_landmarker.FaceLandmarkerOptions) -> 'FaceLandmarker' from builtins.type\n",
      " |      Creates the `FaceLandmarker` object from face landmarker options.\n",
      " |      \n",
      " |      Args:\n",
      " |        options: Options for the face landmarker task.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `FaceLandmarker` object that's created from `options`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If failed to create `FaceLandmarker` object from\n",
      " |          `FaceLandmarkerOptions` such as missing the model.\n",
      " |        RuntimeError: If other types of error occurred.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mediapipe.tasks.python.vision.core.base_vision_task_api.BaseVisionTaskApi:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Return `self` upon entering the runtime context.\n",
      " |  \n",
      " |  __exit__(self, unused_exc_type, unused_exc_value, unused_traceback)\n",
      " |      Shuts down the mediapipe vision task instance on exit of the context manager.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If the mediapipe vision task failed to close.\n",
      " |  \n",
      " |  __init__(self, graph_config: mediapipe.framework.calculator_pb2.CalculatorGraphConfig, running_mode: mediapipe.tasks.python.vision.core.vision_task_running_mode.VisionTaskRunningMode, packet_callback: Optional[Callable[[Mapping[str, mediapipe.python._framework_bindings.packet.Packet]], NoneType]] = None) -> None\n",
      " |      Initializes the `BaseVisionTaskApi` object.\n",
      " |      \n",
      " |      Args:\n",
      " |        graph_config: The mediapipe vision task graph config proto.\n",
      " |        running_mode: The running mode of the mediapipe vision task.\n",
      " |        packet_callback: The optional packet callback for getting results\n",
      " |          asynchronously in the live stream mode.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: The packet callback is not properly set based on the task's\n",
      " |        running mode.\n",
      " |  \n",
      " |  close(self) -> None\n",
      " |      Shuts down the mediapipe vision task instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If the mediapipe vision task failed to close.\n",
      " |  \n",
      " |  convert_to_normalized_rect(self, options: mediapipe.tasks.python.vision.core.image_processing_options.ImageProcessingOptions, image: mediapipe.python._framework_bindings.image.Image, roi_allowed: bool = True) -> mediapipe.tasks.python.components.containers.rect.NormalizedRect\n",
      " |      Converts from ImageProcessingOptions to NormalizedRect, performing sanity checks on-the-fly.\n",
      " |      \n",
      " |      If the input ImageProcessingOptions is not present, returns a default\n",
      " |      NormalizedRect covering the whole image with rotation set to 0. If\n",
      " |      'roi_allowed' is false, an error will be returned if the input\n",
      " |      ImageProcessingOptions has its 'region_of_interest' field set.\n",
      " |      \n",
      " |      Args:\n",
      " |        options: Options for image processing.\n",
      " |        image: The image to process.\n",
      " |        roi_allowed: Indicates if the `region_of_interest` field is allowed to be\n",
      " |          set. By default, it's set to True.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A normalized rect proto that represents the image processing options.\n",
      " |  \n",
      " |  get_graph_config(self) -> mediapipe.framework.calculator_pb2.CalculatorGraphConfig\n",
      " |      Returns the canonicalized CalculatorGraphConfig of the underlying graph.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from mediapipe.tasks.python.vision.core.base_vision_task_api.BaseVisionTaskApi:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(landmarker)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
